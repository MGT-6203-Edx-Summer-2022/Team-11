---
title: "MoM Linear Regression"
author: "David Chang"
date: "7/10/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries and Data

```{r libraries and data}

#setwd("C:/Users/David/Desktop/EdX/MGT6203/Group Project/Team-11")

#-------------------------------------------------------------------------------
# Libraries
#-------------------------------------------------------------------------------
library(tidyverse)
library(corrplot)
library(lubridate)
library(anytime)
library(car)
library(leaps)
library(caret)
library(glmnet)
#-------------------------------------------------------------------------------
# Load Monthly Data
#-------------------------------------------------------------------------------
# Load  data
Macro_SP_MoM <- read.csv("Data preparation/Macro_SP_MoM.csv", 
                     fileEncoding="UTF-8-BOM")

# converting dates to date format
# Macro_SP_MoM <- as.Date(anytime(Macro_SP_MoM$date))

# Remove Date column and Shiller_PE
Macro_SP_MoM <- subset(Macro_SP_MoM,select=-c(date))
Macro_SP_MoM <- Macro_SP_MoM[-21]

# check data
head(Macro_SP_MoM)
summary(Macro_SP_MoM)
```

## Exploratory Data Analysis

```{r EDA}
# histogram, QQ Plot, Box plot of SP500
hist(Macro_SP_MoM$SP500, xlab='S&P monthly returns', 
     main='S&P monthly return distribution from 1967 to 2021',
     breaks = 40, col='lightblue', ylim=c(0,100))
qqnorm(Macro_SP_MoM$SP500)
boxplot(Macro_SP_MoM$SP500, las=1, horizontal = TRUE, main='SP500 boxplot')

# remove outliers
SP_outliers <- c(501, 637, 250,249)
SP500_adj <- Macro_SP_MoM$SP500[-SP_outliers]

# check for normality and outliers
qqnorm(SP500_adj)
shapiro.test(SP500_adj)
```

```{r}
corrplot(cor(Macro_SP_MoM[names(Macro_SP_MoM)!='SP500']), order='AOE')

# check VIF for each independent variables

model <- lm(SP500~., Macro_SP_MoM)
summary(model)
plot(model)

# plot vif and find vif value higher than 5
barplot(vif(model), main='VIF values', col='lightblue', las =2)
abline(h=5, lwd=3,lty=2)

# solutions to address multicolinearity and adjusted datasets

sol1_removed <- c('UNRATE', 'GDP', 'PAYEMS', 'BOGZ1FA895050005Q')
sol2_removed <- c('UNRATE', 'GDP', 'GDPC1')
sol3_removed <- c('PAYEMS', 'GDP', 'GDPC1')
Macro_SP_MoM_sol1 <- Macro_SP_MoM[,-which(colnames(Macro_SP_MoM) %in% sol1_removed)]
Macro_SP_MoM_sol2 <- Macro_SP_MoM[,-which(colnames(Macro_SP_MoM) %in% sol2_removed)]
Macro_SP_MoM_sol3 <- Macro_SP_MoM[,-which(colnames(Macro_SP_MoM) %in% sol3_removed)]

```
```
```{r}

library(gridExtra)

#summary(Macro_SP_MoM)
p1 <- qplot(PPIACO, SP500, data = Macro_SP_MoM)
p2 <- qplot(INDPRO, SP500, data = Macro_SP_MoM)
p3 <- qplot(PAYEMS, SP500, data = Macro_SP_MoM)
p4 <- qplot(WTISPLC, SP500, data = Macro_SP_MoM)
p5 <- qplot(GDP, SP500, data = Macro_SP_MoM)
p6 <- qplot(CPIAUCSL, SP500, data = Macro_SP_MoM)
p7 <- qplot(BOGZ1FA895050005Q, SP500, data = Macro_SP_MoM)
p8 <- qplot(GDPC1, SP500, data = Macro_SP_MoM)
p9 <- qplot(UNRATE, SP500, data = Macro_SP_MoM)
p10 <- qplot(PRS85006023, SP500, data = Macro_SP_MoM)
p11 <- qplot(UMCSENT, SP500, data = Macro_SP_MoM)
p12 <- qplot(DFF, SP500, data = Macro_SP_MoM)
p13 <- qplot(CPILFESL, SP500, data = Macro_SP_MoM)
p14 <- qplot(DSPIC96, SP500, data = Macro_SP_MoM)
p15 <- qplot(PCE, SP500, data = Macro_SP_MoM)
p16 <- qplot(PERMIT, SP500, data = Macro_SP_MoM)
p17 <- qplot(W068RCQ027SBEA, SP500, data = Macro_SP_MoM)
p18 <- qplot(DGS10, SP500, data = Macro_SP_MoM)
p19 <- qplot(GFDEBTN, SP500, data = Macro_SP_MoM)
p20 <- qplot(ICSA, SP500, data = Macro_SP_MoM)
p21 <- qplot(DXY, SP500, data = Macro_SP_MoM)
grid.arrange(p1, p2, p3, nrow = 1)
grid.arrange(p4, p5, p6,  nrow = 1)
grid.arrange(p7, p8, p9,  nrow = 1)
grid.arrange(p10, p11, p12,  nrow = 1)
grid.arrange(p13, p14, p15,  nrow = 1)
grid.arrange(p16, p17, p18,  nrow = 1)
grid.arrange(p19, p20, p21,  nrow = 1)

histogram(~ SP500, Macro_SP_MoM)

```

## Stepwise Linear Regression (original dataset)

```{r stepwise regression}
#create null and full models
m0 <- lm(SP500 ~ 1, data = Macro_SP_MoM)
m1 <- lm(SP500 ~ ., data = Macro_SP_MoM)

```

```{r forward}
forward <- step(m0, scope = list(lower = m0, upper = m1),
                direction = "forward", k = log(NROW(Macro_SP_MoM$SP500)), trace = 0)
summary(forward)
forward$anova
extractAIC(forward) 
# with BIC, p = 6 predictors
# keep Shiller_PE, CPIAUCSL, PAYEMS, ICSA, PPIACO, INDPRO

# with AIC, p = 12 predictors, adjusted R Squared .98
```

```{r backward}

backward <- step(m1, scope = list(lower = m0, upper = m1),
                direction = "backward", k = log(NROW(Macro_SP_MoM$SP500)), trace = 0)
summary(backward)
backward$anova
extractAIC(backward) 
# with BIC, p = 9 predictors
# keep Shiller_PE, CPIAUCSL, PAYEMS, ICSA, PPIACO
# as well as GDP, GDPC1, DFF, GFDEBTN

#with AIC, similar model result as forward, p = 12
```

```{r both}
both <- step(m0, scope = list(lower = m0, upper = m1),
                direction = "both", k = log(NROW(Macro_SP_MoM$SP500)), trace = 0)
summary(both)
both$anova
extractAIC(both) 
#with BIC, 6 predictors, same model as forward selection
#with AIC less variables with stepwise in both directions, p = 10, no change to adjusted R squared
```

```{r exhaustive search using leaps}

leaps_model <- regsubsets(Macro_SP_MoM$SP500 ~ ., 
                                 data = Macro_SP_MoM, nvmax = 19)
s <- summary(leaps_model)
print(s)

plot(leaps_model, scale = "bic")

plot(s$adjr2, xlab = "No. of variables", ylab = "Adjusted R Squared")
points(which.max(s$adjr2), s$adjr2[which.max(s$adjr2)],pch = 19, col = 'red')

# no real difference in adjusted R squared based on num of predictors

plot(s$cp, xlab = "No. of variables", ylab = "AIC")
points(which.min(s$cp), s$cp[which.min(s$cp)],pch = 19, col = 'red')

# Using Mallow's Criterion, p = 10 predictor variables

plot(s$bic, xlab = "No. of variables", ylab = "BIC")
points(which.min(s$bic), s$bic[which.min(s$bic)],pch = 19, col = 'red')

# Using BIC, p = 9 predictor variables

final_leaps <- regsubsets(Macro_SP_MoM$SP500 ~ ., 
                                 data = Macro_SP_MoM, nvmax = 9)
plot(final_leaps, scale = "bic")
# keep PPIACO, PAYEMS, GDP, CPIAUCSL, GDP1, DFF, GFDEBTN, ICSA, Shiller_PE
# summary(final_leaps)
# this is same model as backward stepwise regression model using BIC

#leaps_6 <- regsubsets(Macro_SP_MoM$SP500 ~ ., data = Macro_SP_MoM, nvmax = 6)
#plot(leaps_6, scale = "bic")
# keep PAYEMS, CPIAUCSL, GDP, GDPC1, ICSA, Shiller_PE
# summary(leaps_6)
```


## Stepwise Linear Regression (sol1 data  set)

```{r stepwise regression}
#create null and full models
m0 <- lm(SP500 ~ 1, data = Macro_SP_MoM_sol1)
m1 <- lm(SP500 ~ ., data = Macro_SP_MoM_sol1)

```

```{r forward}
forward <- step(m0, scope = list(lower = m0, upper = m1),
                direction = "forward", k = log(NROW(Macro_SP_MoM_sol1$SP500)), trace = 0)
summary(forward)
forward$anova
extractAIC(forward) 
# with BIC, p = 7 predictors, adj R squared 0.98
# keep Shiller_PE, CPIAUCSL, PCE, ICSA, PPIACO, DSPIC96, GFDEBTN
```

```{r backward}

backward <- step(m1, scope = list(lower = m0, upper = m1),
                direction = "backward", k = log(NROW(Macro_SP_MoM_sol1$SP500)), trace = 0)
summary(backward)
backward$anova
extractAIC(backward) 
# with BIC, p = 7 predictors. 0.98 adj R2
# same model as forward selection
```

```{r both}
both <- step(m0, scope = list(lower = m0, upper = m1),
                direction = "both", k = log(NROW(Macro_SP_MoM_sol1$SP500)), trace = 0)
summary(both)
both$anova
extractAIC(both) 
# same model as foward and backward selection
```

```{r exhaustive search using leaps}

leaps_model <- regsubsets(Macro_SP_MoM_sol1$SP500 ~ ., 
                                 data = Macro_SP_MoM_sol1, nvmax = 19)
s <- summary(leaps_model)
print(s)

plot(leaps_model, scale = "bic")

plot(s$adjr2, xlab = "No. of variables", ylab = "Adjusted R Squared")
points(which.max(s$adjr2), s$adjr2[which.max(s$adjr2)],pch = 19, col = 'red')

# no real difference in adjusted R squared based on num of predictors

plot(s$cp, xlab = "No. of variables", ylab = "AIC")
points(which.min(s$cp), s$cp[which.min(s$cp)],pch = 19, col = 'red')

# Using Mallow's Criterion, p = 10 predictor variables, same as original dataset

plot(s$bic, xlab = "No. of variables", ylab = "BIC")
points(which.min(s$bic), s$bic[which.min(s$bic)],pch = 19, col = 'red')

# Using BIC, p = 7 predictor variables, same as stepwise regression

final_leaps <- regsubsets(Macro_SP_MoM_sol1$SP500 ~ ., 
                                 data = Macro_SP_MoM_sol1, nvmax = 7)
plot(final_leaps, scale = "bic")
# keep PPIACO, PAYEMS, GDP, CPIAUCSL, GDP1, DFF, GFDEBTN, ICSA, Shiller_PE
# summary(final_leaps)
# same model as stepwise regression
```

## Stepwise Linear Regression (sol2 data  set)

```{r stepwise regression}
#create null and full models
m0 <- lm(SP500 ~ 1, data = Macro_SP_MoM_sol2)
m1 <- lm(SP500 ~ ., data = Macro_SP_MoM_sol2)

```

```{r forward}
forward <- step(m0, scope = list(lower = m0, upper = m1),
                direction = "forward", k = log(NROW(Macro_SP_MoM_sol2$SP500)), trace = 0)
summary(forward)
forward$anova
extractAIC(forward) 
# with BIC, p = 6 predictors, adj R squared 0.98
# keep Shiller_PE, CPIAUCSL, PAYEMS, ICSA, PPIACO, INDPRO
```

```{r backward}

backward <- step(m1, scope = list(lower = m0, upper = m1),
                direction = "backward", k = log(NROW(Macro_SP_MoM_sol2$SP500)), trace = 0)
summary(backward)
backward$anova
extractAIC(backward) 
# with BIC, p = 9 predictors. 0.98 adj R2
# keep Shiller_PE, CPIAUCSL, PAYEMS, ICSA, PPIACO
# and DFF, DSPIC96, W068RCQ027SBEA, GFDEBTN
```

```{r both}
both <- step(m0, scope = list(lower = m0, upper = m1),
                direction = "both", k = log(NROW(Macro_SP_MoM_sol2$SP500)), trace = 0)
summary(both)
both$anova
extractAIC(both) 
# same model as foward
```

```{r exhaustive search using leaps}

leaps_model <- regsubsets(Macro_SP_MoM_sol2$SP500 ~ ., 
                                 data = Macro_SP_MoM_sol2, nvmax = 19)
s <- summary(leaps_model)
print(s)

plot(leaps_model, scale = "bic")

plot(s$adjr2, xlab = "No. of variables", ylab = "Adjusted R Squared")
points(which.max(s$adjr2), s$adjr2[which.max(s$adjr2)],pch = 19, col = 'red')

# no real difference in adjusted R squared based on num of predictors

plot(s$cp, xlab = "No. of variables", ylab = "AIC")
points(which.min(s$cp), s$cp[which.min(s$cp)],pch = 19, col = 'red')

# Using Mallow's Criterion, p = 11 predictor variables, same as original dataset

plot(s$bic, xlab = "No. of variables", ylab = "BIC")
points(which.min(s$bic), s$bic[which.min(s$bic)],pch = 19, col = 'red')

# Using BIC, p = 6 predictor variables, same as stepwise regression

final_leaps <- regsubsets(Macro_SP_MoM_sol2$SP500 ~ ., 
                                 data = Macro_SP_MoM_sol2, nvmax = 6)
plot(final_leaps, scale = "bic")
# summary(final_leaps)
# same model as 6 var stepwise regression model
```

## Stepwise Linear Regression (sol3 data  set)

```{r stepwise regression}
#create null and full models
m0 <- lm(SP500 ~ 1, data = Macro_SP_MoM_sol3)
m1 <- lm(SP500 ~ ., data = Macro_SP_MoM_sol3)

```

```{r forward}
forward <- step(m0, scope = list(lower = m0, upper = m1),
                direction = "forward", k = log(NROW(Macro_SP_MoM_sol3$SP500)), trace = 0)
summary(forward)
forward$anova
extractAIC(forward) 
# with BIC, p = 5 predictors, adj R squared 0.98
# keep Shiller_PE, CPIAUCSL, UNRATE, ICSA, PPIACO
```

```{r backward}

backward <- step(m1, scope = list(lower = m0, upper = m1),
                direction = "backward", k = log(NROW(Macro_SP_MoM_sol3$SP500)), trace = 0)
summary(backward)
backward$anova
extractAIC(backward) 
# with BIC, p = 8 predictors. 0.98 adj R2
# keep Shiller_PE, CPIAUCSL, UNRATE, ICSA, PPIACO
# and DSPIC96, W068RCQ027SBEA, GFDEBTN
```

```{r both}
both <- step(m0, scope = list(lower = m0, upper = m1),
                direction = "both", k = log(NROW(Macro_SP_MoM_sol3$SP500)), trace = 0)
summary(both)
both$anova
extractAIC(both) 
# same model as foward selection
```

```{r exhaustive search using leaps}

leaps_model <- regsubsets(Macro_SP_MoM_sol3$SP500 ~ ., 
                                 data = Macro_SP_MoM_sol3, nvmax = 19)
s <- summary(leaps_model)
print(s)

plot(leaps_model, scale = "bic")

plot(s$adjr2, xlab = "No. of variables", ylab = "Adjusted R Squared")
points(which.max(s$adjr2), s$adjr2[which.max(s$adjr2)],pch = 19, col = 'red')

# no real difference in adjusted R squared based on num of predictors

plot(s$cp, xlab = "No. of variables", ylab = "AIC")
points(which.min(s$cp), s$cp[which.min(s$cp)],pch = 19, col = 'red')

# Using Mallow's Criterion, p = 12 predictor variables, same as original dataset

plot(s$bic, xlab = "No. of variables", ylab = "BIC")
points(which.min(s$bic), s$bic[which.min(s$bic)],pch = 19, col = 'red')

# Using BIC, p = 5 predictor variables, same as stepwise regression

final_leaps <- regsubsets(Macro_SP_MoM_sol3$SP500 ~ ., 
                                 data = Macro_SP_MoM_sol3, nvmax = 5)
plot(final_leaps, scale = "bic")
# summary(final_leaps)
# same model as stepwise regression
```

## Final Stepwise models

```{r}
#helper function

test_resample = function(fit, data) {
  set.seed(42)
  index <- createDataPartition(data$SP500, p = 0.9, times = 1, list = FALSE)
  train <- data[index,]
  test <- data[-index,]
  pred <- predict(fit, test)
  postResample(pred, test$SP500)
}

```


```{r}

Macro_SP_MoM_0m0 <- lm(SP500 ~ 1, data = Macro_SP_MoM)
Macro_SP_MoM_0m1 <- lm(SP500 ~ ., data = Macro_SP_MoM)

sol0_model <- step(Macro_SP_MoM_0m1, scope = list(lower = Macro_SP_MoM_0m0, upper = Macro_SP_MoM_0m1),
                direction = "both", k = log(NROW(Macro_SP_MoM$SP500)), trace = 0)
#summary(sol0_model)

Macro_SP_MoM_1m0 <- lm(SP500 ~ 1, data = Macro_SP_MoM_sol1)
Macro_SP_MoM_1m1 <- lm(SP500 ~ ., data = Macro_SP_MoM_sol1)

sol1_model <- step(Macro_SP_MoM_1m1 , scope = list(lower = Macro_SP_MoM_1m0 , upper = Macro_SP_MoM_1m1),
                direction = "both", k = log(NROW(Macro_SP_MoM_sol1$SP500)), trace = 0)
#summary(sol1_model)

Macro_SP_MoM_2m0 <- lm(SP500 ~ 1, data = Macro_SP_MoM_sol2)
Macro_SP_MoM_2m1 <- lm(SP500 ~ ., data = Macro_SP_MoM_sol2)

sol2_model <- step(Macro_SP_MoM_2m1, scope = list(lower = Macro_SP_MoM_2m0, upper = Macro_SP_MoM_2m1),
                direction = "both", k = log(NROW(Macro_SP_MoM_sol2$SP500)), trace = 0)
#summary(sol2_model)

Macro_SP_MoM_3m0 <- lm(SP500 ~ 1, data = Macro_SP_MoM_sol3)
Macro_SP_MoM_3m1 <- lm(SP500 ~ ., data = Macro_SP_MoM_sol3)

sol3_model <- step(Macro_SP_MoM_3m1, scope = list(lower = Macro_SP_MoM_3m0, upper = Macro_SP_MoM_3m1),
                direction = "both", k = log(NROW(Macro_SP_MoM_sol3$SP500)), trace = 0)
#summary(sol3_model)

df <- data.frame("Model" = c("Stepwise Orig Data", "Stepwise Sol 1",
                             "Stepwise Sol 2", "Stepwise Sol 3",
                             "ElasticNet Orig Data", "ElasticNet Sol 1", 
                             "ElasticNet Sol 2", "ElasticNet Sol 3"),
                 "RMSE" = rep(0,8),
                 "RSquared" = rep(0,8),
                 "MAE" = rep(0,8))

df[1,2:4] <- test_resample(sol0_model, Macro_SP_MoM)
df[2,2:4] <- test_resample(sol1_model, Macro_SP_MoM_sol1)
df[3,2:4] <- test_resample(sol2_model, Macro_SP_MoM_sol2)
df[4,2:4] <- test_resample(sol3_model, Macro_SP_MoM_sol3)

```

## ElasticNet Linear Regression Models

```{r}
set.seed(42)

cv_5 = trainControl(method = "cv", number = 5)

get_best_result = function(caret_fit) {
  best = which(rownames(caret_fit$results) == rownames(caret_fit$bestTune))
  best_result = caret_fit$results[best, ]
  rownames(best_result) = NULL
  best_result
}

```

```{r sol0 ElasticNet}

set.seed(42)

sol0_elnet = train(
  SP500 ~ ., data = Macro_SP_MoM,
  method = "glmnet", preProc = c("center", "scale"),
  trControl = cv_5, tuneLength = 10)

print(sol0_elnet)

get_best_result(sol0_elnet)

#best alpha is 0.7, very small lambda
```

```{r sol0 variable selection}


X_sol0 = model.matrix(SP500 ~ ., data = Macro_SP_MoM)[,-1]
Y_sol0 = Macro_SP_MoM$SP500

set.seed(42)

fit_lasso_cv_sol0 <- cv.glmnet(X_sol0, Y_sol0, 
                               alpha = get_best_result(sol0_elnet)[1,1], 
                               family = "gaussian", standardize = TRUE)
#fit_lasso_cv_sol0
coef(fit_lasso_cv_sol0) # 1 variables kept
# kept intercept only
sum(coef(fit_lasso_cv_sol0) != 0) # num of predictors that are not 0
sum(coef(fit_lasso_cv_sol0) == 0) # num of variables where coef = 0
```

```{r sol1 ElasticNet}

set.seed(42)

sol1_elnet = train(
  SP500 ~ ., data = Macro_SP_MoM_sol1,
  method = "glmnet", preProc = c("center", "scale"),
  trControl = cv_5, tuneLength = 10)

print(sol1_elnet)

get_best_result(sol1_elnet)

#best alpha is 0.1, very small lambda
```

```{r sol1 variable selection}


X_sol1 = model.matrix(SP500 ~ ., data = Macro_SP_MoM_sol1)[,-1]
Y_sol1 = Macro_SP_MoM_sol1$SP500

set.seed(42)

fit_lasso_cv_sol1 <- cv.glmnet(X_sol1, Y_sol1, 
                               alpha = get_best_result(sol1_elnet)[1,1], 
                               family = "gaussian", standardize = TRUE)
#fit_lasso_cv_sol1
coef(fit_lasso_cv_sol1) # 1 variables kept
# keep intercept
sum(coef(fit_lasso_cv_sol1) != 0) # num of predictors that are not 0
sum(coef(fit_lasso_cv_sol1) == 0) # num of variables where coef = 0
```

```{r sol2 ElasticNet}

set.seed(42)

sol2_elnet = train(
  SP500 ~ ., data = Macro_SP_MoM_sol2,
  method = "glmnet", preProc = c("center", "scale"),
  trControl = cv_5, tuneLength = 10)

print(sol2_elnet)

get_best_result(sol2_elnet)

#best alpha is 0.1, very small lambda
```

```{r sol2 variable selection}


X_sol2 = model.matrix(SP500 ~ ., data = Macro_SP_MoM_sol2)[,-1]
Y_sol2 = Macro_SP_MoM_sol2$SP500

set.seed(42)

fit_lasso_cv <- cv.glmnet(X_sol2, Y_sol2, 
                          alpha = get_best_result(sol2_elnet)[1,1], 
                          family = "gaussian", standardize = TRUE)
#fit_lasso_cv
coef(fit_lasso_cv) # 2 variables kept
# keep UMCSENT, ICSA
sum(coef(fit_lasso_cv) != 0) # num of predictors that are not 0
sum(coef(fit_lasso_cv) == 0) # num of variables where coef = 0
```


```{r sol3 ElasticNet}

set.seed(42)

sol3_elnet = train(
  SP500 ~ ., data = Macro_SP_MoM_sol3,
  method = "glmnet", preProc = c("center", "scale"),
  trControl = cv_5, tuneLength = 10)

print(sol3_elnet)

get_best_result(sol3_elnet)

#best alpha is 0.7, very small lambda
```

```{r sol3 variable selection}


X_sol3 = model.matrix(SP500 ~ ., data = Macro_SP_MoM_sol3)[,-1]
Y_sol3 = Macro_SP_MoM_sol3$SP500

set.seed(42)

fit_lasso_cv <- cv.glmnet(X_sol3, Y_sol3, 
                          alpha = get_best_result(sol3_elnet)[1,1], 
                          family = "gaussian", standardize = TRUE)
#fit_lasso_cv
coef(fit_lasso_cv) # 1 variables kept
# keep intercept
sum(coef(fit_lasso_cv) != 0) # num of predictors that are not 0
sum(coef(fit_lasso_cv) == 0) # num of variables where coef = 0
```

```{r test elasticnet models}

#sol 0 stepwise
#p = 5, keep PAYEMS, UMCSENT, PCE, ICSA, DXY
#summary(sol0_model)

#sol 1 stepwise
#p = 3, keep UMCSENT, ICSA, DXY
#summary(sol1_model)

#sol 2 stepwise
#p = 5, keep PAYEMS, UMCSENT, PCE, ICSA, DXY
#summary(sol2_model)

#sol 3 stepwise
#p = 5, keep UNRATE, UMCSENT, PCE, ICSA, DXY
#summary(sol3_model)

#sol 0 EL
df[5,2:4] <- test_resample(sol0_elnet, Macro_SP_MoM)
# p = 0, intercept only

#sol 1 EL
df[6,2:4] <- test_resample(sol1_elnet, Macro_SP_MoM_sol1)
# p = 0, intercept only

#sol 2 EL
df[7,2:4] <- test_resample(sol2_elnet, Macro_SP_MoM_sol2)
# p = 2, UMCSENT, ICSA

#sol 3 EL
df[8,2:4] <- test_resample(sol3_elnet, Macro_SP_MoM_sol3)
# p = 0, intercept only

print(df)
```
